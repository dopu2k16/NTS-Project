# -*- coding: utf-8 -*-
"""Bengali_Abstractive_Summarization_DL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ys59QiMuSUgSk4rmFZ0IQL9zH-h_ACxQ

# Import libraries and mount Google Drive on Colab

In this section, we discuss about the packages needed to be installed in order to run the experiments for the project. We listed down the required packages for the project. We also discuss as to how to mount the Google Drive on Google Colab and also how to upload data to Colab Notebook.

## Required packages:
* nltk
* gensim
* wget
* pickle
* tensorflow
* numpy
* matplotlib
* tempfile
* wget
* tarfile
* gzip
* zipfile
"""

# Commented out IPython magic to ensure Python compatibility.
# Selecting the tensorflow 1 version for our project
# %tensorflow_version 1.x

# Installing nltk, pyrouge, gensim, wget packages
!pip install nltk
!pip install pyrouge
!pip install gensim
!pip install wget
  
import nltk
nltk.download('punkt')

#Installing package for unzipping files
!sudo apt-get install p7zip-full p7zip-rar

#Unzipping files
!7z e data_50k.7z

# Importing libraries for the project
import os,re,sys,codecs,string
from importlib import reload
import tensorflow.compat.v1 as tf 
from tensorflow.contrib import rnn
import tensorflow as tf
tf.disable_v2_behavior()
tf.disable_eager_execution()

import re
from nltk.corpus import stopwords
import time
from tensorflow.python.layers.core import Dense
from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors
print('TensorFlow Version: {}'.format(tf.__version__))
import warnings
warnings.filterwarnings('ignore')
from nltk.tokenize import word_tokenize
import re
import collections
import pickle
import numpy as np
from gensim.models.keyedvectors import KeyedVectors
from gensim.test.utils import get_tmpfile
from gensim.scripts.glove2word2vec import glove2word2vec
import gensim
import tempfile
import wget
import os
import tarfile
import gzip
import zipfile
import argparse
#tf.disable_v2_behavior()
#tf.disable_eager_execution()
tf.compat.v1.disable_eager_execution()

#adding logging
import logging

logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt = '%m/%d/%Y %H:%M:%S',
                    level = logging.INFO)
logger = logging.getLogger("bert")

import json

#Mounting google drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
directory_path = '/content/gdrive/My Drive/Bengali Summarization/'
# %cd /content/gdrive/My Drive/Bengali Summarization/

#Uploading files to the drive
from google.colab import files
uploaded = files.upload()

"""# Data Preprocessing

In this section, we discuss about the data preprocessing steps such as removing unwarranted characters, English alphabets, various punctuations, Bengali stopwords, etc. in order to clean the dataset for training and validation purposes.
"""

# Reading bengali stopwords from a file
with open('stopwords_list_ben.txt','r') as g:
    lines=g.readlines()
    stopwords_bn= [word for line in lines for word in line.split()]

# cleaning the text, performing data cleaning
def clean_text(text, remove_stopwords = True):
    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''
                
    # Format words and remove unwanted characters
    text = re.sub(r'https?:\/\/.*[\r\n]*', '', str(text), flags=re.MULTILINE)
    text = re.sub(r'\<a href', ' ', str(text))
    text = re.sub(r'&amp;', '', str(text)) 
    text = re.sub(r'[_"\-;%()|+&=*%.,!?:#$@\[\]/]', ' ', str(text))
    text = re.sub(r'<br />', ' ', str(text))
    text = re.sub(r'\'', '', str(text))
    text = re.sub(r"[a-zA-Z0-9_.-]",' ', str(text))
    whitespace = re.compile(u"[\s\u0020\u00a0\u1680\u180e\u202f\u205f\u3000\u2000-\u200a]+", re.UNICODE)
    bangla_fullstop = u"\u0964"
    punctSeq   = u"['\"“”‘’]+|[.?!,…]+|[:;]+"
    punc = u"[(),$%^&*+={}\[\]:\"|\'\~`<>/,¦!?½£¶¼©⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟↉¤¿º;-]+"
    text= whitespace.sub(" ",text).strip()
    text = re.sub(punctSeq, " ", text)
    text = re.sub(bangla_fullstop, " ",text)
    text = re.sub(punc, " ", text)
        
    if remove_stopwords:
        stopped_tokens=[i for i in text.split() if not i in stopwords_bn]
        #stopped_tokens=[re.sub(r'[^\W\d_,;]',' ', i) for i in stopped_tokens]
        #stopped_tokens=[re.sub(r'[\w\s]','',i, re.UNICODE) for i in stopped_tokens]
        #stopped_tokens=[re.sub(r"[\u09E6\u09E7\u09E8\u09E9\u09EA\u09EB\u09EC\u09ED\u09EE\u09EF]",' ', i) for i in stopped_tokens]
        stopped_tokens=[re.sub(r"[\u09E4-\u09EF\u0964\09FB\u201C\u201D\u2018\u2019\u10191]",'', str(i)) for i in stopped_tokens]
        stopped_tokens=[re.sub(r"[’,‘‘!:?;""'-(''’’`)]",'', str(i)) for i in stopped_tokens]
        stopped_tokens=[re.sub(r"[a-zA-Z0-9_.-]",'', str(i)) for i in stopped_tokens]
        text = text.split()
        text = [w for w in text if not w in stopped_tokens]
        text = " ".join(text)

    return text

# Reading the dataset va pandas
import pandas as pd
import matplotlib.pyplot as plt
df= pd.read_csv('/content/gdrive/My Drive/Bengali Summarization/data_50k.csv', encoding='utf-8')

# This is how the dataset looks like: title is the ground truth for summary, article as the content
df

df.describe()

df.title.describe()

df.content.describe()

# Clean the summaries and texts
clean_summaries = []
for summary in df.title:
    clean_summaries.append(clean_text(summary, remove_stopwords=False))
print("Summaries are complete.")

clean_texts = []
for text in df.content:
    clean_texts.append(clean_text(text, remove_stopwords=False))
print("Texts are complete.")

# Displaying the cleaned article from which the model will generate summaries
clean_texts[0]

clean_summaries

# Inspect the cleaned summaries and texts to ensure they have been cleaned well
for i in range(5):
    print("Clean Review #",i+1)
    print(clean_summaries[i])
    print(clean_texts[i])
    print()

"""# Train Word2vec on dataset

In this section, we train Word2vec modle on our dataset. This generates word embeddings for Bengali.
"""

# We merge both the articles and the sumamaries and train word2vec on the entire dataset
clean_documents_list = [clean_texts[i].split() for i in range(len(clean_texts))] + [clean_summaries[i].split() for i in range(len(clean_summaries))]
len(clean_documents_list)

#clean_documents_list = [clean_texts[i].split() for i in clean_text] + [clean_summaries[i].split() for i in clean_summaries]
len(clean_documents_list)
import gensim
word2vec_beng_vec = gensim.models.Word2Vec(
        clean_documents_list,
        size=300,
        window=10,
        min_count=1, sg=1)
#word2vec_beng_vec.train(clean_documents_list, total_examples=len(clean_documents_list), epochs=10)
word2vec_beng_vec.save(directory_path +"word2vec_bengali.bin")
#word2vec_beng_vec.wv.save('drive/My Drive/Colab Notebooks/Model 4_5/' +"model_arabic_extreme.model")

"""# Data Statistics

In this section, we try to find the size of the vocabulary, the length of the embedding matrix, the percentage of missing words from the vocabulary, the count of <UNK> tokens in the summaries and the news articles. We also built dictionary where we map words to their respective positions and the vice-versa.
"""

# Creating a dictionary of words with their frequency
def count_words(count_dict, text):
    '''Count the number of occurrences of each word in a set of text'''
    
    for sentence in text:
        for word in sentence.split():
            if word not in count_dict:
                count_dict[word] = 1
            else:
                count_dict[word] += 1

# Find the number of times each word was used and the size of the vocabulary
word_counts = {}

count_words(word_counts, clean_summaries)
count_words(word_counts, clean_texts)
            
print("Size of Vocabulary:", len(word_counts))

#Creating an embedding matrix by loading the word vectors 
from gensim.models import Word2Vec
embeddings_index = {}
embedding = Word2Vec.load('word2vec_bengali.bin')
for word in embedding.wv.vocab:
    #values = embedding.wv.word_vec(word)
    #embeddings_index[word] = np.asarray(values)
    embeddings_index[word] = embedding.wv.word_vec(word)

print('Word embeddings:', len(embeddings_index))

# Embedding vector of a Bengali word
embeddings_index['সংসদে']

# Find the number of words that are missing from the vocabulary, and are used more than our threshold. 
#Though its not necessary as we didn't use any pretrained word embeddings as we have trained the word embeddingds on the dataset
missing_words = 0
threshold = 20

for word, count in word_counts.items():
    if count > threshold:
        if word not in embeddings_index:
            missing_words += 1
            
missing_ratio = round(missing_words/len(word_counts),4)*100
            
print("Number of words missing:", missing_words)
print("Percent of words that are missing from vocabulary: {}%".format(missing_ratio))

# Calculating number of unique words, extending the vocabulary with specialtokens such as "<UNK>","<PAD>","<EOS>","<GO>" to mark the  beginning, end of sequences 
# Creating dctionary mapping words to integers and the vice-versa
#dictionary to convert words to integers
vocab_to_int = {} 

value = 0
for word, count in word_counts.items():
    if count >= threshold or word in embeddings_index:
        vocab_to_int[word] = value
        value += 1

# Special tokens that will be added to our vocab
codes = ["<UNK>","<PAD>","<EOS>","<GO>"]   

# Add codes to vocab
for code in codes:
    vocab_to_int[code] = len(vocab_to_int)

# Dictionary to convert integers to words
int_to_vocab = {}
for word, value in vocab_to_int.items():
    int_to_vocab[value] = word

usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100

print("Total number of unique words:", len(word_counts))
print("Number of words we will use:", len(vocab_to_int))
print("Percent of words we will use: {}%".format(usage_ratio))

len(vocab_to_int)

#mapping words to their respective positions
vocab_to_int

# Need to use 300 for embedding dimensions to match CN's vectors.
embedding_dim = 300
nb_words = len(vocab_to_int)

# Create matrix with default values of zero
word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)
for word, i in vocab_to_int.items():
    if word in embeddings_index:
        word_embedding_matrix[i] = embeddings_index[word]
    else:
        # If word not in embedding matrix,, create a random embedding for it
        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))
        embeddings_index[word] = new_embedding
        word_embedding_matrix[i] = new_embedding

# Check if value matches len(vocab_to_int)
print(len(word_embedding_matrix))

def convert_to_ints(text, word_count, unk_count, eos=False):
    '''Convert words in text to an integer.
       If word is not in vocab_to_int, use UNK's integer.
       Total the number of words and UNKs.
       Add EOS token to the end of texts'''
    ints = []
    for sentence in text:
        sentence_ints = []
        for word in sentence.split():
            word_count += 1
            if word in vocab_to_int:
                sentence_ints.append(vocab_to_int[word])
            else:
                sentence_ints.append(vocab_to_int["<UNK>"])
                unk_count += 1
        if eos:
            sentence_ints.append(vocab_to_int["<EOS>"])
        ints.append(sentence_ints)
    return ints, word_count, unk_count

# Apply convert_to_ints to clean_summaries and clean_texts
# Calculating the number of unkown tokens in articles and summaries 
word_count = 0
unk_count = 0

int_summaries, word_count_summaries, unk_count_summaries = convert_to_ints(clean_summaries, word_count, unk_count)
int_texts, word_count_texts, unk_count_texts = convert_to_ints(clean_texts, word_count, unk_count, eos=True)

unk_percent_summaries = round(unk_count_summaries/word_count_summaries,4)*100
unk_percent_texts = round(unk_count_texts/word_count_texts,4)*100
print("Total number of words in summaries:", word_count_summaries)
print("Total number of UNKs in summaries:", unk_count_summaries)
print("Percent of words that are UNK in summaries: {}%".format(unk_percent_summaries))
print("Total number of words in texts:", word_count_texts)
print("Total number of UNKs in texts:", unk_count_texts)
print("Percent of words that are UNK in texts: {}%".format(unk_percent_texts))

def unk_counter(sentence):
    '''Counts the number of time UNK appears in a sentence.'''
    unk_count = 0
    for word in sentence:
        if word == vocab_to_int["<UNK>"]:
            unk_count += 1
    return unk_count

"""# Insights about the length of the summary and article in the dataset

In this section, we find out the $n^{th}$ percentile of the length of both the summaries and the news articles for summarization. We also sorted the summaries and news articles by the length of the texts, shortest to longest and by predefined maximum summary and article lengths, <UNK> counts in summaries and text of the news articles.
"""

def create_lengths(text):
    '''Create a data frame of the sentence lengths from a text'''
    lengths = []
    for sentence in text:
        #print(sentence)
        lengths.append(len(sentence))
    return pd.DataFrame(lengths, columns=['counts'])

# Inspect the length of texts and finding the nth percentile of the length of the news articles
lengths_texts = create_lengths(clean_texts)
print(np.percentile(lengths_texts.counts, 50))
print(np.percentile(lengths_texts.counts, 90))
print(np.percentile(lengths_texts.counts, 95))
print(np.percentile(lengths_texts.counts, 99))

lengths_texts.describe()

# Inspect the length of summaries and finding the nth percentile of the length of the news titles(summaries)
lengths_summary = create_lengths(clean_summaries)
print(np.mean(lengths_summary))
print(np.percentile(lengths_summary.counts, 50))
print(np.percentile(lengths_summary.counts, 90))
print(np.percentile(lengths_summary.counts, 95))
print(np.percentile(lengths_summary.counts, 99))

lengths_summary.describe()

# Sort the summaries and texts by the length of the texts, shortest to longest
# Limit the length of summaries and texts based on the min and max ranges.
# Remove reviews that include too many UNKs
# Setting the max length of the summary to be 30 and max length of the article to be 400 due to memory constraints for training
sorted_summaries = []
sorted_texts = []
max_text_length = 400
max_summary_length = 30
min_length = 5
unk_text_limit = 20
unk_summary_limit =10
sorted_summaries_txt, sorted_texts_txt = [], []
for length in range(min(lengths_texts.counts), max_text_length): 
    for count, words in enumerate(int_summaries):
        if (len(int_summaries[count]) >= min_length and
            len(int_summaries[count]) <= max_summary_length and
            len(int_texts[count]) >= min_length and
            unk_counter(int_summaries[count]) <= unk_summary_limit and
            unk_counter(int_texts[count]) <= unk_text_limit and
            length == len(int_texts[count])
           ):
            sorted_summaries.append(int_summaries[count])
            sorted_summaries_txt.append(clean_summaries[count])
            sorted_texts.append(int_texts[count])
            sorted_texts_txt.append(clean_texts[count])
        
# Compare lengths to ensure they match
print(len(sorted_summaries))
print(len(sorted_texts))

"""# Splitting dataset into train and validation datasets respectively

Here, we split the entire dataset into training and validation set respectively. We followed 80/20% split strategy.
"""

#Splititng the dataset in order to use the real ground truth summaries and article for calculating the ROUGE scores for the generated summaries
from sklearn.model_selection import train_test_split
train_text, valid_text, train_summary, valid_summary = train_test_split(np.array(sorted_texts_txt), np.array(sorted_summaries_txt), test_size=0.2, random_state=0, shuffle=True)

# Spitting the dataset into training and validation set
from sklearn.model_selection import train_test_split
train_x, valid_x, train_y, valid_y = train_test_split(np.array(int_texts), np.array(int_summaries), test_size=0.2, random_state=0, shuffle=True)

# Priting deatils about GPU on Colab
from tensorflow.python.client import device_lib 
print(device_lib.list_local_devices())

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print(
      '\n\nThis error most likely means that this notebook is not '
      'configured to use a GPU.  Change this in Notebook Settings via the '
      'command palette (cmd/ctrl-shift-P) or the Edit menu.\n\n')
  raise SystemError('GPU device not found')
tf.test.gpu_device_name()

"""# Model

In this section we define the model architecture and define the various functions to build the dataset after padding, and adding the special tokens for encoder and decoder parts of the model. We also loop throug the dataset in batches as shown in the function <code>batch_iter()</code>.
"""

# Defining the function to build the dictionary and the processed dataset (adding special tokens such as <GO>, <EOS>, <PAD>, <UNK> for the seq2seq model)
from sklearn.model_selection import train_test_split
def build_dict(step, toy=False):
    if step == "train":
        train_text_list = train_text.tolist()
        train_summary_list = train_summary.tolist()

        words = list()
        for sent in train_summary_list + train_text_list:
            for word in sent.split():
                words.append(word)

        word_counter = collections.Counter(words).most_common()
        word_dict = dict()
        word_dict["<PAD>"] = 0
        word_dict["<UNK>"] = 1
        word_dict["<GO>"] = 2
        word_dict["<EOS>"] = 3
        for word, _ in word_counter:
            word_dict[word] = len(word_dict)

        with open(directory_path + "word_dict.pickle", "wb") as f:
            pickle.dump(word_dict, f)

    elif step == "valid":
        with open(directory_path + "word_dict.pickle", "rb") as f:
            word_dict = pickle.load(f)

    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))

    article_max_len = 400
    summary_max_len = 30

    return word_dict, reversed_dict, article_max_len, summary_max_len


def build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False):
    if step == "train":
        article_list = train_text.tolist()
        summary_list = train_summary.tolist()
    elif step == "valid":
        article_list = valid_text.tolist()
    else:
        raise NotImplementedError

    x = [[w for w in d.split()] for d in article_list]
    x = [[word_dict.get(w, word_dict["<UNK>"]) for w in d] for d in x]
    x = [d[:article_max_len] for d in x]
    x = [d + (article_max_len - len(d)) * [word_dict["<PAD>"]] for d in x]
    
    if step == "valid":
        return x
    else:        
        y = [d.split() for d in summary_list]
        y = [[word_dict.get(w, word_dict["<UNK>"]) for w in d] for d in y]
        y = [d[:(summary_max_len - 1)] for d in y]
        return x, y


def batch_iter(inputs, outputs, batch_size, num_epochs):
    inputs = np.array(inputs)
    outputs = np.array(outputs)

    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1
    for epoch in range(num_epochs):
        for batch_num in range(num_batches_per_epoch):
            start_index = batch_num * batch_size
            end_index = min((batch_num + 1) * batch_size, len(inputs))
            yield inputs[start_index:end_index], outputs[start_index:end_index]


def get_init_embedding(reversed_dict, embedding_size):
    #glove_file = default_path + "glove/glove.6B.300d.txt"
    word2vec_file = get_tmpfile(directory_path + "word2vec_bengali.txt.vectors.npy")
    #glove2word2vec(glove_file, word2vec_file)
    print("Loading Word2vec vectors for Bengali...")
    #word_vectors = KeyedVectors.load_word2vec_format(word2vec_file, unicode_errors='ignore')
    #word_vectors = Word2Vec.load("word2vec_bengali.txt.vectors.npy")
    #word_vectors = KeyedVectors.load('word2vec_bengali.txt', mmap='r')
    word_vectors = Word2Vec.load('word2vec_bengali.bin')

    #with open(directory_path + "word2vec_bengali.txt.vectors.npy", 'rb') as handle:
    #    word_vectors = pickle.load(handle.read())
        
    word_vec_list = list()
    for _, word in sorted(reversed_dict.items()):
        try:
            word_vec = word_vectors.wv.word_vec(word)
        except KeyError:
            word_vec = np.zeros([embedding_size], dtype=np.float32)

        word_vec_list.append(word_vec)

    # Assign random vector to <GO>, <EOS> token
    word_vec_list[2] = np.random.normal(0, 1, embedding_size)
    word_vec_list[3] = np.random.normal(0, 1, embedding_size)

    return np.array(word_vec_list)

# Implemented from the tensoflow implementation of Seq2seq model available on Github
import tensorflow as tf
from tensorflow.contrib import rnn
from gensim.models import Word2Vec

class Model(object):
    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):
        self.vocabulary_size = len(reversed_dict)
        self.embedding_size = args.embedding_size
        self.num_hidden = args.num_hidden
        self.num_layers = args.num_layers
        self.learning_rate = args.learning_rate
        self.beam_width = args.beam_width
        if not forward_only:
            self.keep_prob = args.keep_prob
        else:
            self.keep_prob = 1.0
        self.cell = tf.nn.rnn_cell.BasicLSTMCell
        with tf.variable_scope("decoder/projection"):
            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)

        self.batch_size = tf.placeholder(tf.int32, (), name="batch_size")
        self.X = tf.placeholder(tf.int32, [None, None])
        self.X_len = tf.placeholder(tf.int32, [None])
        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])
        self.decoder_len = tf.placeholder(tf.int32, [None])
        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])
        self.global_step = tf.Variable(0, trainable=False)

        with tf.name_scope("embedding"):
            if not forward_only and args.glove:
                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)
            else:
                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)
            self.embeddings = tf.get_variable("embeddings", initializer=init_embeddings)
            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])
            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])

        with tf.name_scope("encoder"):
            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]
            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]
            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]
            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]

            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(
                fw_cells, bw_cells, self.encoder_emb_inp,
                sequence_length=self.X_len, time_major=True, dtype=tf.float32)
            self.encoder_output = tf.concat(encoder_outputs, 2)
            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)
            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)
            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)

        with tf.name_scope("decoder"), tf.variable_scope("decoder") as decoder_scope:
            decoder_cell = self.cell(self.num_hidden * 2)

            if not forward_only:
                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])
                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(
                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)
                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,
                                                                   attention_layer_size=self.num_hidden * 2)
                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)
                initial_state = initial_state.clone(cell_state=self.encoder_state)
                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)
                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)
                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)
                self.decoder_output = outputs.rnn_output
                self.logits = tf.transpose(
                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])
                self.logits_reshape = tf.concat(
                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)
            else:
                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(
                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)
                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)
                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)
                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(
                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)
                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,
                                                                   attention_layer_size=self.num_hidden * 2)
                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)
                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)
                decoder = tf.contrib.seq2seq.BeamSearchDecoder(
                    cell=decoder_cell,
                    embedding=self.embeddings,
                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),
                    end_token=tf.constant(3),
                    initial_state=initial_state,
                    beam_width=self.beam_width,
                    output_layer=self.projection_layer
                )
                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(
                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)
                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])

        with tf.name_scope("loss"):
            if not forward_only:
                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(
                    logits=self.logits_reshape, labels=self.decoder_target)
                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)
                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))

                params = tf.trainable_variables()
                gradients = tf.gradients(self.loss, params)
                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)
                optimizer = tf.train.AdamOptimizer(self.learning_rate)
                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)

"""# Training

In this section, we define the training loop for our model along with setting the hyperparamters. We also save the model after every epoch if needed or else after the mentioned epochs of training is done. We also report the training loss log across the number of iterations.
"""

default_path = '/content/gdrive/My Drive/Bengali Summarization/'

import time
start = time.perf_counter()
import tensorflow as tf
import argparse
import pickle
import os

# Setting the hyperparameters
# --num_hidden" : "Network size"
# --num_layers": "Network depth"  
# --beam_width": "Beam width for beam search decoder"
#--learning_rate": "Learning rate"
# batch_size": "Batch size."
#"--num_epochs": "Number of epochs"
#"--keep_prob": "Dropout keep prob"
#--toy", action="store_true"
#"--with_model", action="store_true" : "Continue from previously saved model"

class args:
    pass
  
args.num_hidden=400
args.num_layers=3
args.beam_width=10
args.glove="store_true"
args.embedding_size=300
args.learning_rate=1e-3
args.batch_size=64
args.num_epochs=30
args.keep_prob = 0.5
args.toy=False #"store_true"
args.with_model=False

if not os.path.exists(directory_path + "saved_model"):
    os.mkdir(directory_path + "saved_model")
else:
    if args.with_model:
        old_model_checkpoint_path = open(directory_path + 'saved_model/checkpoint', 'r')
        old_model_checkpoint_path = "".join([directory_path + "saved_model/", old_model_checkpoint_path.read().splitlines()[0].split('"')[1] ])


print("Building dictionary...")
word_dict, reversed_dict, article_max_len, summary_max_len = build_dict("train", args.toy)
print("Loading training dataset...")
train_x, train_y = build_dataset("train", word_dict, article_max_len, summary_max_len, args.toy)

tf.reset_default_graph()
tloss = []
with tf.Session() as sess:
    model = Model(reversed_dict, article_max_len, summary_max_len, args)
    sess.run(tf.global_variables_initializer())
    saver = tf.train.Saver(tf.global_variables())
    if 'old_model_checkpoint_path' in globals():
        print("Continuing from previous trained model:" , old_model_checkpoint_path , "...")
        #saver.restore(sess, old_model_checkpoint_path )

    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)
    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1

    print("\nIteration starts.")
    print("Number of batches per epoch :", num_batches_per_epoch)
    for batch_x, batch_y in batches:
        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))
        batch_decoder_input = list(map(lambda x: [word_dict["<GO>"]] + list(x), batch_y))
        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))
        batch_decoder_output = list(map(lambda x: list(x) + [word_dict["<EOS>"]], batch_y))

        batch_decoder_input = list(
            map(lambda d: d + (summary_max_len - len(d)) * [word_dict["<PAD>"]], batch_decoder_input))
        batch_decoder_output = list(
            map(lambda d: d + (summary_max_len - len(d)) * [word_dict["<PAD>"]], batch_decoder_output))

        train_feed_dict = {
            model.batch_size: len(batch_x),
            model.X: batch_x,
            model.X_len: batch_x_len,
            model.decoder_input: batch_decoder_input,
            model.decoder_len: batch_decoder_len,
            model.decoder_target: batch_decoder_output
        }

        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)
        tloss.append(loss)

        if step % 100 == 0:
            print("step {0}: loss = {1}".format(step, loss))

        if step % num_batches_per_epoch == 0:
            if ((step // num_batches_per_epoch) == args.num_epochs):
                hours, rem = divmod(time.perf_counter() - start, 3600)
                minutes, seconds = divmod(rem, 60)
                saver.save(sess, default_path + "saved_model/model.ckpt", global_step=step)
                print(" Epoch {0}: Model is saved.".format(step // num_batches_per_epoch),
                      "Elapsed: {:0>2}:{:0>2}:{:05.2f}".format(int(hours),int(minutes),seconds) , "\n")

"""## Training loss plot
In this section, we visualize the training loss of various experiments. It is seen from the figure below that the training loss is converging as the number of epochs increases.
"""

import matplotlib.pyplot as plt
plt.plot(tloss)
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Training loss')
plt.savefig('loss_ep10_3layers-drop5.png')
plt.show()

"""# Validation

Here, we run the validation loop to generate abstractive summaries from the model. We also defiend the hyperparamters for decoding the decoder for the summaries generated. We also save the generated summaries in a text file so that we can access it in future.
"""

import tensorflow as tf
import pickle

tf.reset_default_graph()

class args:
    pass

# Defining the hyperparameters for validation  
# Consider just one epoch as you just go once throught the entire validation set
args.num_hidden=400
args.num_layers=3
args.beam_width=10
args.glove="store_true"
args.embedding_size=300
args.learning_rate=1e-3
args.batch_size=64
args.keep_prob = 0.5
args.toy=True
args.with_model="store_true"

print("Loading dictionary...")
word_dict, reversed_dict, article_max_len, summary_max_len = build_dict("valid", args.toy)
print("Loading validation dataset...")
valid_x = build_dataset("valid", word_dict, article_max_len, summary_max_len, args.toy)
valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]
print("Loading article and reference...")
article = valid_text
reference = valid_summary

with tf.Session() as sess:
    print("Loading saved model...")
    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)
    saver = tf.train.Saver(tf.global_variables())
    #saver = tf.train.import_meta_graph('/content/gdrive/My Drive/Bengali Summarization/saved_model/model.ckpt-909.meta')
    #saver.restore(sess,tf.train.latest_checkpoint(default_path))
    ckpt = tf.train.get_checkpoint_state(default_path + "saved_model/")
    #saver.restore(sess, '/content/gdrive/My Drive/Bengali Summarization/saved_model/model.ckpt-909.meta')
    saver.restore(sess, ckpt.model_checkpoint_path)

    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)

    print("Writing summaries to 'result.txt'...")
    for batch_x, _ in batches:
        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]

        valid_feed_dict = {
            model.batch_size: len(batch_x),
            model.X: batch_x,
            model.X_len: batch_x_len,
        }

        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)
        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]
        summary_array = []
        with open(default_path + "result-ep10-3layers-dp5.txt", "a") as f:
            for line in prediction_output:
                summary = list()
                for word in line:
                    if word == "<EOS>":
                        break
                    if word not in summary:
                        summary.append(word)
                summary_array.append(" ".join(summary))
                print(" ".join(summary), file=f)

    print('Summaries have been generated')

len(prediction_output)

"""# Evaluate

Here we evaluate the ROUGE metrics (ROUGE-1, ROUGE-2, and ROUGE-L) for the summaries generated.
"""

# Installling the official rouge package to calculate the ROUGE scores
!pip install rouge

# Saving the ground truth summary in a text file to evaluate generated summaries from the decoder
article = valid_text
reference = valid_summary
with open('/content/gdrive/My Drive/Bengali Summarization/reference.txt',mode='w') as f:
    for line in reference:
        print("".join(line), file=f)

from rouge import FilesRouge
from rouge import Rouge 
# Calculating ROUGE scores across all generated summaries from the validation set
files_rouge = FilesRouge()
scores = files_rouge.get_scores('/content/gdrive/My Drive/Bengali Summarization/result-ep20-3layers.txt', '/content/gdrive/My Drive/Bengali Summarization/reference.txt')

reference[73]

article[4]

scores

#  Calculating the average ROUGE scores across all the summaries generated from the validation set
scores_avg = files_rouge.get_scores('/content/gdrive/My Drive/Bengali Summarization/result-ep10-3layers-dp5.txt', '/content/gdrive/My Drive/Bengali Summarization/reference.txt', avg=True)
print(scores_avg)

# Reading the generated summaries from the file
file = open('/content/gdrive/My Drive/Bengali Summarization/result-3.txt', 'r')
# read all text
text = file.read()
summary_arr = []
for sent in text.split('\n'):
    summary_arr.append(sent)

len(summary_arr)